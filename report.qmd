---
title: "Creating code embedding matrix"
subtitle: "Midterm project for PHS7045"
format: pdf
editor: visual
author: Yidan Zhang
---

# Introduction

Word embedding has become a significant topic in natural language processing, especially when working with context data. The key concept behind word embedding is based on the idea that a word can be characterized by "the company it keeps," or its context. Word embeddings capture this contextual information by creating a lower-dimensional space (compared to traditional one-hot encoding) where words that appear in similar contexts are positioned close to one another. This allows for more efficient and effective word representation in models.

One of the most widely recognized methods for generating word embeddings is Word2Vec, developed by Google in 2013. This neural network-based approach is trained on large corpora of text, learning to predict the context of a word based on its surrounding words within a local window. While Word2Vec focuses on local context, another prominent method, GloVe (Global Vectors for Word Representation), expands this concept by incorporating global context. GloVe takes advantage of the overall word-to-word co-occurrence patterns across the entire corpus. By using a precomputed, count-based co-occurrence matrix, GloVe leverages global statistical information about how often words appear together to generate its embedding matrix.

With the increasing digitization of patient information, electronic health records (EHR) have become essential for efficiently tracking medical histories and facilitating the exchange of data between healthcare providers. As structured data within EHR systems, ICD (International Classification of Diseases) and CPT (Current Procedural Terminology) codes have long been used to record diagnoses and procedures, respectively, even before the widespread adoption of EHRs. Embedding these medical codes offers several key benefits. It can enhance disease prediction and diagnosis modeling by allowing models to recognize similarities between related conditions, improving predictive performance. Additionally, ICD code embeddings support patient clustering, enabling algorithms like k-means or DBSCAN to group patients based on diagnostic patterns and improve clustering effectiveness. They also help identify complex comorbidity patterns and hidden phenotypes that traditional methods may miss. Finally, code embedding aids in healthcare data integration, which is especially valuable given the challenges of accessing protected data due to data safety concerns.

The goal of this project is to implement the GloVe algorithm to create a package that generates word embedding matrices from EHR data. The package will be tested on the MIMIC-III clinical dataset to generate ICD-9 code embeddings.

# Solution Plan

-   STEP 1:

    The first step in generating embeddings using the GloVe approach is to create a code co-occurrence matrix, $\mathbf{X}$, where each element of $\mathbf{Xâ€‹_{ij}}$ represents how often word $i$ appears in context of word $j$. The function's arguments will include the user's dataset (`data`), along with parameters to specify which column corresponds to the subject in dataset (`id`), which column represents the time or sequence information for defining the context window (`time`), and the user-defined size of the context window (`window`). EHR data typically consists of large datasets with vast amounts of patient information, and each patient have multiple diagnosis codes. To compute the co-occurrences, the function must iterate through each patient and count the co-occurrence of ICD codes within a user-defined context window,with approximately 13,000 ICD-9 codes, this process can be both time-consuming and computationally expensive. To address these challenges, the project will optimize performance by coercing the data to a `data.table` structure for faster reference handling, applying vectorization for efficient matrix generation, and incorporating parallel computing to further improve time efficiency.

-   STEP 2:

    The second step is to use the co-occurrence matrix to create the pointwise mutual information (PMI) matrix, which measures the association between a word and a context word. PMI is calculated as $\text{PMI}(w,c) = \frac{p(w, c)}{p(w) \cdot p(c)}$ , where $p(w,c)$ is the count of how often word $w$ and context code $c$ occur in the same context window, divided by the total count of code pairs within the window; and $p(w)$ and $p(c)$ are singleton frequency of the word $w$ and $c$, respectively, within the context window. In machine learning literature, the shifted positive pointwise mutual information (SPPMI) matrix is commonly used, where PMI is shifted by a user-defined constant $log(k)$. To accommodate user-specific needs, another function in the package will be aim to calculate SPPMI, which is defined as $\text{SPPMI}(w,c)=max(\text{PMI}(w,c)-log(k),0)$. The computation starts by obtaining the joint and singleton frequencies for the codes using the function `get_SG`, where vectorization is applied. The two functions will then be used to compute both PMI and SPPMI, respectively.

-   STEP 3:

    In this step, truncated singular value decomposition (SVD) is applied to factorize the PMI/SPPMI matrix and generate lower-dimensional embeddings for the codes. The function allows the user to specify the number of top $p$-dimensional singular values and vectors to retain, as well as the maximum number of iterations for the SVD process. Once executed, the function will return the code embedding matrix $\mathbf{W}= u \cdot \sqrt{d}$ , where the matrix has a dimensionality reduced to the user-defined vector length $p$. To optimize the code's performance, vectorization techniques and the use of `data.table` will be incorporated to accelerate the process and ensure efficient computation.

-   STEP 4 (Time Permitting):

    One approach to help users determine the optimal embedding vector length is to evaluate the performance of the embedding matrix by predicting phecode-based classification accuracy. Phecodes group ICD codes into clinically meaningful categories, and an ideal embedding matrix would accurately capture diagnostic information, allowing for precise classification aligned with phecodes. By assessing the AUC (Area Under the Curve) for different vector lengths, users can identify the length that maximizes performance, minimizing information loss and avoiding overfitting.

# Preliminary Result

-   Data

    The data used to test the package is sourced from the [MIMIC-III clinical dataset](https://physionet.org/content/mimiciii/1.4/), a freely available database containing de-identified health-related data from patients who were admitted to the ICU at Beth Israel Deaconess Medical Center between 2001 and 2012. The dataset includes diagnoses recorded using the 9th version of the ICD coding system, specifically in a table named "DIAGNOSES_ICD." A brief summary of this data is provided below:

    ![Summary of the table](images/clipboard-684336803.png){width="354"}

    The data used to generate this report is available on the repository and is a random sample of 100 subjects from the original data.

    ```{r}
    #| message: false
    #| include: false
    library(data.table)
    library(tidyverse)
    ```

-   Function

    The first step is to create a function for the co-occurrence matrix, firstly, we can create a function without using any idea of optimization, and that is simply create an empty matrix with dimension as the number of unique code in the data, and go through each patient for the count.

```{r}
#| echo: false
cooccur_old <- function(data, id, code, time, window = NA) {
  # Get unique ICD codes from the data to 
  unique_codes <- unique(data[[code]])
  code_count <- length(unique_codes)
  
  # Build a matrix filled with 0 as a start point for the counts
  cooccurrence_matrix <- matrix(0, nrow = code_count, ncol = code_count)
  rownames(cooccurrence_matrix) <- unique_codes
  colnames(cooccurrence_matrix) <- unique_codes
  
  # Iterate through each patient
  patients <- unique(data[[id]])
  for (patient in patients) {
    # Get data for the specific patient and sort by time column (SEQ_NUM)
    patient_data <- data %>%
      filter(!!sym(id) == patient) %>%
      arrange(!!sym(time))
    
    # Skip patients with fewer than 2 observations
    if (nrow(patient_data) < 2) {
      next  # Skip this patient if there's only one or no observations
    }
    
    # If window is NA, set window to be the number of rows for this patient
    if (is.na(window)) {
      patient_window <- nrow(patient_data)
    } else {
      patient_window <- window
    }
    
    # For each ICD code in patient data, compare with others within the window
    for (i in 1:(nrow(patient_data) - 1)) {
      for (j in (i + 1):nrow(patient_data)) {
        if (patient_data[[time]][j] - patient_data[[time]][i] > patient_window) break
        
        code_i <- patient_data[[code]][i]
        code_j <- patient_data[[code]][j]
        
        # Update the co-occurrence matrix
        cooccurrence_matrix[code_i, code_j] <- cooccurrence_matrix[code_i, code_j] + 1
        cooccurrence_matrix[code_j, code_i] <- cooccurrence_matrix[code_j, code_i] + 1
      }
    }
  }
  
  return(cooccurrence_matrix)
}

# Example usage:
# matrix <- cooccur_old(data = your_data, id = "SUBJECT_ID", code = "ICD9_CODE", time = "SEQ_NUM", window = NA)


```

```{r}
library(data.table)

cooccur_optimized <- function(data, id, code, time, window = NA) {
  # Convert to data.table for faster processing
  setDT(data)
  
  # Get unique ICD codes and map them to matrix indices
  unique_codes <- unique(data[[code]])
  code_count <- length(unique_codes)
  code_to_index <- setNames(seq_along(unique_codes), unique_codes)
  
  # Initialize an empty co-occurrence matrix
  cooccurrence_matrix <- matrix(0, nrow = code_count, ncol = code_count)
  rownames(cooccurrence_matrix) <- unique_codes
  colnames(cooccurrence_matrix) <- unique_codes
  
  # Process each patient
  patients <- unique(data[[id]])
  
  for (patient in patients) {
    # Filter and sort data for the patient
    patient_data <- data[get(id) == patient, .(time_value = get(time), code_value = get(code))][order(time_value)]
    
    # Skip patients with fewer than 2 observations
    if (nrow(patient_data) < 2) next
    
    # Set window size
    patient_window <- if (is.na(window)) nrow(patient_data) else window
    
    # Vectorized computation of co-occurrence within the window
    time_diffs <- outer(patient_data$time_value, patient_data$time_value, "-")
    within_window <- which(time_diffs >= 0 & time_diffs <= patient_window, arr.ind = TRUE)
    
    # Update the co-occurrence matrix
    for (pair in 1:nrow(within_window)) {
      i <- within_window[pair, 1]
      j <- within_window[pair, 2]
      if (i >= j) next  # Avoid redundant pairs or self-pairs
      
      code_i <- patient_data$code_value[i]
      code_j <- patient_data$code_value[j]
      
      index_i <- code_to_index[[code_i]]
      index_j <- code_to_index[[code_j]]
      
      # Update co-occurrence counts
      cooccurrence_matrix[index_i, index_j] <- cooccurrence_matrix[index_i, index_j] + 1
      cooccurrence_matrix[index_j, index_i] <- cooccurrence_matrix[index_j, index_i] + 1
    }
  }
  
  return(cooccurrence_matrix)
}

```

-   

-   The execution of the functions

```{r}
data <- fread("https://raw.githubusercontent.com/yidanzh0518/project/main/data/sample_data.csv")



# Benchmark both functions
benchmark_results <- microbenchmark(
  old_version = cooccur_old(data = sample_data, id = "SUBJECT_ID", code = "ICD9_CODE", time = "SEQ_NUM", window = NA),
  optimized_version = cooccur_optimized(data = sample_data, id = "SUBJECT_ID", code = "ICD9_CODE", time = "SEQ_NUM", window = NA),
  times = 5  # Number of times to run each function for more accurate comparison
)

# Print the benchmark results
print(benchmark_results)

# Optional: Plot the results for a visual comparison
boxplot(benchmark_results)

```
